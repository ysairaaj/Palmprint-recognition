{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfab1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b75fcc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## transformation grid generator##\n",
    "def affine_grid_generator(height, width, theta):\n",
    "    \"\"\"\n",
    "    This function returns a sampling grid, which when\n",
    "    used with the bilinear sampler on the input feature\n",
    "    map, will create an output feature map that is an\n",
    "    affine transformation [1] of the input feature map.\n",
    "    Input\n",
    "    -----\n",
    "    - height: desired height of grid/output. Used\n",
    "      to downsample or upsample.\n",
    "    - width: desired width of grid/output. Used\n",
    "      to downsample or upsample.\n",
    "    - theta: affine transform matrices of shape (num_batch, 2, 3).\n",
    "      For each image in the batch, we have 6 theta parameters of\n",
    "      the form (2x3) that define the affine transformation T.\n",
    "    Returns\n",
    "    -------\n",
    "    - normalized grid (-1, 1) of shape (num_batch, 2, H, W).\n",
    "      The 2nd dimension has 2 components: (x, y) which are the\n",
    "      sampling points of the original image for each point in the\n",
    "      target image.\n",
    "    Note\n",
    "    ----\n",
    "    [1]: the affine transformation allows cropping, translation,\n",
    "         and isotropic scaling.\n",
    "    \"\"\"\n",
    "    num_batch = tf.shape(theta)[0]\n",
    "\n",
    "    # create normalized 2D grid\n",
    "    x = tf.linspace(-1.0, 1.0, width)\n",
    "    y = tf.linspace(-1.0, 1.0, height)\n",
    "    x_t, y_t = tf.meshgrid(x, y)\n",
    "\n",
    "    # flatten\n",
    "    x_t_flat = tf.reshape(x_t, [-1])\n",
    "    y_t_flat = tf.reshape(y_t, [-1])\n",
    "\n",
    "    # reshape to [x_t, y_t , 1] - (homogeneous form)\n",
    "    ones = tf.ones_like(x_t_flat)\n",
    "    sampling_grid = tf.stack([x_t_flat, y_t_flat, ones])\n",
    "\n",
    "    # repeat grid num_batch times\n",
    "    sampling_grid = tf.expand_dims(sampling_grid, axis=0)\n",
    "    sampling_grid = tf.tile(sampling_grid, tf.stack([num_batch, 1, 1]))\n",
    "\n",
    "    # cast to float32 (required for matmul)\n",
    "    theta = tf.cast(theta, 'float32')\n",
    "    sampling_grid = tf.cast(sampling_grid, 'float32')\n",
    "\n",
    "    # transform the sampling grid - batch multiply\n",
    "    batch_grids = tf.matmul(theta, sampling_grid)\n",
    "    # batch grid has shape (num_batch, 2, H*W)\n",
    "\n",
    "    # reshape to (num_batch,2, H, W)\n",
    "    batch_grids = tf.reshape(batch_grids, [num_batch, 2, height, width])\n",
    "    #print(batch_grids)\n",
    "    return batch_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af972645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilinear_sampler(img, x, y):\n",
    "    \"\"\"\n",
    "    Performs bilinear sampling of the input images according to the\n",
    "    normalized coordinates provided by the sampling grid. Note that\n",
    "    the sampling is done identically for each channel of the input.\n",
    "    To test if the function works properly, output image should be\n",
    "    identical to input image when theta is initialized to identity\n",
    "    transform.\n",
    "    Input\n",
    "    -----\n",
    "    - img: batch of images in (B, H, W, C) layout.\n",
    "    - grid: x, y which is the output of affine_grid_generator.\n",
    "    Returns\n",
    "    -------\n",
    "    - out: interpolated images according to grids. Same size as grid.\n",
    "    \"\"\"\n",
    "    H = tf.shape(img)[1]\n",
    "    W = tf.shape(img)[2]\n",
    "    max_y = tf.cast(H - 1, 'int32')\n",
    "    max_x = tf.cast(W - 1, 'int32')\n",
    "    zero = tf.zeros([], dtype='int32')\n",
    "\n",
    "    # rescale x and y to [0, W-1/H-1]\n",
    "    x = tf.cast(x, 'float32')\n",
    "    y = tf.cast(y, 'float32')\n",
    "    x = 0.5 * ((x + 1.0) * tf.cast(max_x-1, 'float32'))\n",
    "    y = 0.5 * ((y + 1.0) * tf.cast(max_y-1, 'float32'))\n",
    "\n",
    "    # grab 4 nearest corner points for each (x_i, y_i)\n",
    "    x0 = tf.cast(tf.floor(x), 'int32')\n",
    "    x1 = x0 + 1\n",
    "    y0 = tf.cast(tf.floor(y), 'int32')\n",
    "    y1 = y0 + 1\n",
    "\n",
    "    # clip to range [0, H-1/W-1] to not violate img boundaries\n",
    "    x0 = tf.clip_by_value(x0, zero, max_x)\n",
    "    x1 = tf.clip_by_value(x1, zero, max_x)\n",
    "    y0 = tf.clip_by_value(y0, zero, max_y)\n",
    "    y1 = tf.clip_by_value(y1, zero, max_y)\n",
    "\n",
    "    # get pixel value at corner coords\n",
    "    Ia = get_pixel_value(img, x0, y0)\n",
    "    Ib = get_pixel_value(img, x0, y1)\n",
    "    Ic = get_pixel_value(img, x1, y0)\n",
    "    Id = get_pixel_value(img, x1, y1)\n",
    "\n",
    "    # recast as float for delta calculation\n",
    "    x0 = tf.cast(x0, 'float32')\n",
    "    x1 = tf.cast(x1, 'float32')\n",
    "    y0 = tf.cast(y0, 'float32')\n",
    "    y1 = tf.cast(y1, 'float32')\n",
    "\n",
    "    # calculate deltas\n",
    "    wa = (x1-x) * (y1-y)\n",
    "    wb = (x1-x) * (y-y0)\n",
    "    wc = (x-x0) * (y1-y)\n",
    "    wd = (x-x0) * (y-y0)\n",
    "\n",
    "    # add dimension for addition\n",
    "    wa = tf.expand_dims(wa, axis=3)\n",
    "    wb = tf.expand_dims(wb, axis=3)\n",
    "    wc = tf.expand_dims(wc, axis=3)\n",
    "    wd = tf.expand_dims(wd, axis=3)\n",
    "\n",
    "    # compute output\n",
    "    out = tf.add_n([wa*Ia, wb*Ib, wc*Ic, wd*Id])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58918be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##pixel value generator##\n",
    "def get_pixel_value(img, x, y):\n",
    "    \"\"\"\n",
    "    Utility function to get pixel value for coordinate\n",
    "    vectors x and y from a  4D tensor image.\n",
    "    Input\n",
    "    -----\n",
    "    - img: tensor of shape (B, H, W, C)\n",
    "    - x: flattened tensor of shape (B*H*W,)\n",
    "    - y: flattened tensor of shape (B*H*W,)\n",
    "    Returns\n",
    "    -------\n",
    "    - output: tensor of shape (B, H, W, C)\n",
    "    \"\"\"\n",
    "    shape = tf.shape(x)\n",
    "    batch_size = shape[0]\n",
    "    height = shape[1]\n",
    "    width = shape[2]\n",
    "\n",
    "    batch_idx = tf.range(0, batch_size)\n",
    "    batch_idx = tf.reshape(batch_idx, (batch_size, 1, 1))\n",
    "    b = tf.tile(batch_idx, (1, height, width))\n",
    "\n",
    "    indices = tf.stack([b, y, x], 3)\n",
    "\n",
    "    return tf.gather_nd(img, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5412ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "203d444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001_1.JPG\n",
      "002_1.JPG\n",
      "003_2.JPG\n",
      "004_1.JPG\n",
      "005_1.JPG\n",
      "006_1.JPG\n",
      "007_1.JPG\n",
      "008_2.JPG\n",
      "009_1.JPG\n",
      "010_1.JPG\n",
      "(10, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "IMGSIZE = (224, 224 ,3)\n",
    "OUT_IMG_SIZE = (224 ,224 ,3)\n",
    "target_size = [IMGSIZE[:2] ,OUT_IMG_SIZE[:2]]\n",
    "paths = r\"C:\\Users\\hp\\anaconda\\Project_files_Palmprint_detector\\Test_Images\"\n",
    "def cos_mse(y_pred ,y_true):\n",
    "    return 10*keras.losses.cosine_similarity(y_pred ,y_true) + 0.01*keras.losses.MeanSquaredError()(y_pred ,y_true)\n",
    "\n",
    "stn_model = tf.keras.models.load_model('C:/Users/hp/anaconda/Project_files_Palmprint_detector/partial_model_step_5' , custom_objects ={'cos_mse' : cos_mse})\n",
    "count = 1\n",
    "img_arr = []\n",
    "images_0 = os.listdir(paths)\n",
    "for ind in range(len(images_0)):\n",
    "    image_0 = images_0[ind]\n",
    "    print(image_0)\n",
    "    path_target_0 = f\"{paths}\\{image_0}\"\n",
    "    try:\n",
    "        img_0 = img_to_array(load_img(path_target_0 , target_size = target_size[0])).astype(np.uint8)\n",
    "        #tf.keras.preprocessing.image.array_to_img(img_0).show()\n",
    "    except:\n",
    "        count = 3\n",
    "        print('Failed at' + path_target_0 )\n",
    "        print('Retrying ...')\n",
    "        while count > 0:\n",
    "            try:\n",
    "                img_0 = img_to_array(load_img(path_target_0 , target_size = target_size[0])).astype(np.uint8)\n",
    "                print('Error mitigated . Proceeding ...')\n",
    "                break\n",
    "            except:\n",
    "                count -= 1\n",
    "                if count == 0:\n",
    "                    print('Failed to mitigate error . Skipping ...')\n",
    "    if count == 0:\n",
    "        continue\n",
    "    img_arr.append(img_0)\n",
    "\n",
    "img_arr = np.array(img_arr)\n",
    "print(img_arr.shape)\n",
    "\n",
    "im = stn_model(img_arr)    \n",
    "\n",
    "for i in range(len(im)):\n",
    "    tf.keras.preprocessing.image.array_to_img(im[i]).show()\n",
    "    #print(im[i].numpy())\n",
    "    im1 = Image.fromarray(im[i].numpy().astype(np.uint8))\n",
    "    im1.save(f\"your_file_{i}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a6dcaccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "class sample_generator(tf.keras.utils.Sequence):\n",
    "    def __init__(self , X_data ,y_data,label,batch_size):\n",
    "        self.x = X_data\n",
    "        self.y = y_data\n",
    "        self.label = Label\n",
    "        self.batch_size = batch_size\n",
    "    def __len__(self) :\n",
    "        return int(np.ceil(len(self.x)*(len(self.x)-1) / float(self.batch_size)))\n",
    "    def __getitem__(self,idx):\n",
    "        batch_x = [None ,None]\n",
    "        batch_y = [[] ,[]]\n",
    "        print((idx//(len(self.x)-1) + (idx%(len(self.x)-1))*self.batch_size)%(len(self.x)-1) ,(idx//(len(self.x)-1) + (idx%(len(self.x)-1))*self.batch_size)%(len(self.x)-1) + self.batch_size)\n",
    "        batch_x = [np.tile(self.x[idx//(len(self.x)-1)] ,(self.batch_size ,1,1,1)) , self.y[(idx//(len(self.x)-1) + (idx%(len(self.x)-1))*self.batch_size)%(len(self.x)-1) :(idx//(len(self.x)-1) + (idx%(len(self.x)-1))*self.batch_size)%(len(self.x)-1) + self.batch_size,...]]\n",
    "        batch_y[0].extend([self.label.tolist()[idx//(len(self.x)-1)]]*(self.batch_size))\n",
    "        #print([self.label.tolist()[idx//(len(self.x)-1)]]*(self.batch_size) )\n",
    "        batch_y[1].extend(self.label[(idx//(len(self.x)-1) + (idx%(len(self.x)-1))*self.batch_size)%(len(self.x)-1) :(idx//(len(self.x)-1) + (idx%(len(self.x)-1))*self.batch_size)%(len(self.x)-1) + self.batch_size,...].tolist())\n",
    "        #print(self.label[(idx//(len(self.x)-1) + (idx%(len(self.x)-1))*self.batch_size)%(len(self.x)-1) :(((idx//len(self.x)-1)) + (idx%(len(self.x)-1))*self.batch_size)%(len(self.x)-1) + self.batch_size,...].tolist())\n",
    "        batch_y = zip(batch_y[0] ,batch_y[1])\n",
    "        y_arr = [0  if y[0] != y[1] else 1 for y in batch_y]\n",
    "        print(len(batch_x[0]) ,len(batch_x[1]),len(y_arr))\n",
    "        return batch_x , np.array(y_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2a2e9cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1298//1298 + (1298%1298)*32)%1298 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "65b6a8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1298"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((1297//1298) + (1297%1298)*32)%(1298) + 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6135fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code started ...\n",
      "Code executed successfully\n"
     ]
    }
   ],
   "source": [
    "import Data_Loader as DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1fa9552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed atD:\\BTP\\4\\IITD_Palmprint_V1\\Right_Hand\\164_5.JPG and D:\\BTP\\4\\IITD_Palmprint_V1\\Segmented\\Right\\164_5.bmp\n",
      "Retrying ...\n",
      "Failed to mitigate error . Skipping ...\n",
      "Data loaded successfully with 1299 left_hand samples and 911 right_hand samples\n",
      "Data_Loader is cleared .\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "data_loader = DL.data_loader(Base_path = r\"D:\\BTP\\4\\IITD_Palmprint_V1\")\n",
    "data_loader(print_process = False  , in_dim = (IMGSIZE[0],IMGSIZE[1]) ,out_dim = (OUT_IMG_SIZE[0],OUT_IMG_SIZE[1]) )\n",
    "X,y,Label,X2,y2,Label2 = data_loader.build_data()\n",
    "X =X.astype('float16') #X.astype('float32')\n",
    "y = y.astype('float16') # y.astype('float32')\n",
    "data_loader.clear_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4cef3eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['001', '001', '001', ..., '230', '230', '230'], dtype='<U3')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "397f981f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1298"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fcb75798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1299, 224, 224, 3)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4bbc7325",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = sample_generator(X , y,Label,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "156e1880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generator[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c35032ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generator[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "84949e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41e1a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "73a78abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 34\n",
      "32 32 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator[2596][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "73c513db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 224, 224, 3)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator[1][0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0e42fab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 33\n",
      "32 32 32\n"
     ]
    }
   ],
   "source": [
    "tf.keras.preprocessing.image.array_to_img(generator[1298][0][1][20]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd45a980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6ad74d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
